#!/usr/bin/env python3
import logging
import json
from pymongo import MongoClient
from lxml import html
from playwright.sync_api import sync_playwright


class SuttonHybridScraper:
    def __init__(self, mongo_uri="mongodb://localhost:27017/",
                 db_name="sutton", urls_collection="agents", details_collection="agent_details"):
        # MongoDB
        self.client = MongoClient(mongo_uri)
        self.db = self.client[db_name]
        self.urls_collection = self.db[urls_collection]
        self.details_collection = self.db[details_collection]

    def fetch_urls(self):
        """Get all saved agent URLs from MongoDB."""
        return [doc["url"] for doc in self.urls_collection.find({})]

    def scrape_agent(self, page, url):
        logging.info(f"Scraping: {url}")
        page.goto(url, timeout=60000, wait_until="domcontentloaded")

        # --- Parse full HTML with lxml ---
        tree = html.fromstring(page.content())

        # --- STEP 1: Extract JSON-LD ---
        try:
            page.wait_for_selector("script[type='application/ld+json']", state="attached", timeout=30000)
            jsonld_script = page.locator("script[type='application/ld+json']").first.inner_text()
            data_json = json.loads(jsonld_script)
        except Exception as e:
            logging.error(f"JSON-LD load error {url}: {e}")
            return {}

        # Parse JSON-LD
        address = data_json.get("address", {})
        affiliation = data_json.get("affiliation", {})
        languages = [lang.get("name") for lang in data_json.get("knowsLanguage", []) if lang.get("name")]
        area_served = [area.get("name") for area in data_json.get("areaServed", []) if area.get("name")]

        # --- STEP 2: Define XPath  ---
        website = tree.xpath("//a[@class='py-3 text-demibold underline hover:text-red-500']/@href")
        agent_phone_numbers =tree.xpath("//ul[@class='mt-3']/li[2]//text()")
        #print(agent_phone_numbers)
        title = tree.xpath("//h3[@class='block text-xl text-demibold text-red-500']//text()")

        # --- COMBINE RESULTS ---
        data = {
            "profile_url": data_json.get("url", url),
            "first_name": data_json.get("name").split()[0] if data_json.get("name") else None,
            "middle_name": "",#not available
            "last_name": " ".join(data_json.get("name").split()[1:]) if data_json.get("name") else None,
            "image_url": data_json.get("image"),
            "office_name": affiliation.get("name"),
            "address": address.get("streetAddress"),
            "description": data_json.get("description"),
            "languages": languages,
            "social": "",#not available
            "website": website or "",
            "email": data_json.get("email"),
            "title": title[0] if title else "",
            "country": address.get("addressCountry"),
            "city": address.get("addressLocality"),
            "zipcode": address.get("postalCode"),
            "state": address.get("addressRegion"),
            "agent_phone_numbers":agent_phone_numbers[1] if len(agent_phone_numbers)>1 else "",
            "office_phone_numbers": data_json.get("telephone") if data_json.get("telephone") else "",
            "area_served": area_served,
        }
        return data

    def save_detail(self, data):
        """Save agent detail into MongoDB."""
        if not self.details_collection.find_one({"profile_url": data["profile_url"]}):
            self.details_collection.insert_one(data)
            logging.info(f"Inserted details: {data['profile_url']}")
        else:
            logging.debug(f"Duplicate detail skipped: {data['profile_url']}")

    def run(self):
        urls = self.fetch_urls()
        logging.info(f"Found {len(urls)} agent URLs in DB")

        with sync_playwright() as p:
            browser = p.chromium.launch(headless=False)
            context = browser.new_context()
            page = context.new_page()

            for url in urls:
                try:
                    data = self.scrape_agent(page, url)
                    if data:
                        self.save_detail(data)
                except Exception as e:
                    logging.error(f"Error scraping {url}: {e}")

            browser.close()


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    scraper = SuttonHybridScraper()
    scraper.run()
